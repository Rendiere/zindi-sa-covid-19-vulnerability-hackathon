{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renierbotha/anaconda3/envs/zindi/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.cv import cv_score, cv_predict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'holdout_indexes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-99601b95ce6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0madv_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/modelling/adv_weights.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mholdout_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholdout_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'holdout_indexes' is not defined"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/modelling/train.csv', index_col=0)\n",
    "test = pd.read_csv('../data/modelling/test.csv', index_col=0)\n",
    "    \n",
    "adv_weights = pd.read_csv('../data/modelling/adv_weights.csv', index_col=0)\n",
    "\n",
    "print('Train: ',train.shape)\n",
    "print('Test: ',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns='target_pct_vunerable')\n",
    "y = train.target_pct_vunerable\n",
    "\n",
    "# Will use this as local val score and compare with CV score\n",
    "X_val = val.drop(columns='target_pct_vunerable')\n",
    "y_val = val.target_pct_vunerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Processing\n",
    "\n",
    "* Process categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "cat_cols = train.select_dtypes('object').columns.tolist()\n",
    "\n",
    "params = {\n",
    "    \"objective\" : \"regression\", \n",
    "    \"metric\" : \"rmse\",\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_depth\":9,\n",
    "    \"feature_fraction\":0.75,\n",
    "    \"bagging_fraction\":0.75,\n",
    "    \"bagging_freq\" : 2,\n",
    "    \"lambda\" : 0.1,\n",
    "    \"num_threads\" :30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check result on val set\n",
    "enc = TargetEncoder(cols=cat_cols)\n",
    "\n",
    "X_train = enc.fit_transform(X, y)\n",
    "X_val_enc = enc.transform(X_val)\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X_train, y, \n",
    "          eval_set=[(X_val_enc, y_val)],\n",
    "          eval_metric='rmse', \n",
    "          verbose=25, \n",
    "          early_stopping_rounds=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({'n_estimators': model.best_iteration_})\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Validation Set Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(X_val_enc)\n",
    "val_preds_df = pd.DataFrame.from_dict({\n",
    "    'yhat': val_preds,\n",
    "    'y': y_val,\n",
    "    'error': y_val - val_preds\n",
    "})\n",
    "\n",
    "val_preds_df.index = X_val.index\n",
    "\n",
    "val_preds_df = val_preds_df.merge(adv_weights, left_index=True, right_index=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(data=val_preds_df, x='yhat', y='error', hue='prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_preds_df.sort_values(by='prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine validation set back with train set\n",
    "data = pd.concat([train, val], axis=0, sort=False)\n",
    "\n",
    "X = data.drop(columns='target_pct_vunerable')\n",
    "y = data.target_pct_vunerable\n",
    "\n",
    "X = ta_enc.fit_transform(X, y)\n",
    "\n",
    "weights = (adv_weights.loc[X.index].prob).values\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ta_enc.transform(test)\n",
    "\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "sub = pd.DataFrame({'ward': X_test.index, y.name: test_preds})\n",
    "\n",
    "sub.to_csv(f'../data/submissions/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "kf = KFold(n_splits=n_folds, random_state=420, shuffle=True)\n",
    "\n",
    "cv_test_preds = pd.DataFrame(index=test.index, columns=list(range(1,n_folds+1)))\n",
    "\n",
    "i=1\n",
    "for train_idx, val_idx in kf.split(X, y):\n",
    "    \n",
    "    print(i, end=' ')\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    X_train = enc.fit_transform(X_train, y_train)\n",
    "    X_val = enc.transform(X_val)\n",
    "    X_test = enc.transform(test)\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train, \n",
    "              early_stopping_rounds=100, \n",
    "              eval_set=[(X_val, y_val)], \n",
    "              eval_metric='rmse',\n",
    "              verbose=False)\n",
    "\n",
    "    val_preds = model.predict(X_val)\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    cv_test_preds.loc[X_test.index, i] = test_preds\n",
    "    \n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_cols=list(range(1, n_folds+1))\n",
    "cv_test_preds['mean'] = cv_test_preds[fold_cols].mean(axis=1)\n",
    "cv_test_preds['std'] = cv_test_preds[fold_cols].std(axis=1)\n",
    "cv_test_preds.sort_values(by='std').tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = cv_test_preds['mean']\n",
    "sub.name = y.name\n",
    "sub = sub.reset_index()\n",
    "# sub\n",
    "sub.to_csv('../data/submissions/lgbm_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../data/submissions/lgbm_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "\n",
    "# Linear Models\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cross Validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Models Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc, test_proc = process_data(train, test)\n",
    "\n",
    "X = train_proc.drop(columns='target_pct_vunerable')\n",
    "y = train_proc.target_pct_vunerable\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "score = cv_score(GBoost, X, y)\n",
    "print(\"\\n GBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression', n_estimators=500)\n",
    "\n",
    "scores = cv_score(model_lgb, X, y)\n",
    "print(\"\\nLGBM score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_preds = cv_predict(model_lgb, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_preds['residual'] = cv_preds['y'] - cv_preds['yhat']\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.scatterplot(data=cv_preds, x='yhat', y='residual', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=cv_preds, x='y', y='yhat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_preds.sort_values(by='residual', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.stacking import AveragingModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "avg_model = AveragingModels([KRR, model_lgb, model_xgb])\n",
    "\n",
    "scores = rmsle_cv(avg_model, X, y)\n",
    "print(\"\\nLGBM score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lgbm import lightgbm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            \"objective\" : \"regression\", \n",
    "            \"metric\" : \"rmse\",\n",
    "            \"max_depth\":9,\n",
    "            \"feature_fraction\":0.75,\n",
    "            \"bagging_fraction\":0.75,\n",
    "            \"bagging_freq\" : 2,\n",
    "            \"lambda\" : 0.1,\n",
    "            \"num_threads\" :30\n",
    "}\n",
    "\n",
    "lgbm_model = lightgbm_model(train, test, 'target_pct_vunerable', \n",
    "                            feature_names=test.columns, \n",
    "                            feval_metrics=['rmse'], \n",
    "                            maximize=False, \n",
    "                            early_stopping_rounds=50, \n",
    "                            params=params\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
